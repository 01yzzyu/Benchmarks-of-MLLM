# Benchmarks-of-MLLM

##  Summary of 200 MLLM Benchmarks

### Perception&Understanding
#### Comprehensive Evaluation
## Comprehensive Evaluation

| Date       | Keywords   | Institute (first) | Paper                                                                                                                          | Publication | Code | Project |
| ---------- | ---------- | ----------------- | ------------------------------------------------------------------------------------------------------------------------------ | ----------- | ---- | ------- |
| 2024-03-01 | MDVP-Bench | Sun Yat-sen       | [Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want](https://arxiv.org/pdf/2403.20271) |             |      |         |

| [  ](https://arxiv.org/pdf/2403.20271) |   |   |   |
| -------------------------------------- | - | - | - |

| [  ](https://arxiv.org/pdf/2403.20271) |   |   |   |
| -------------------------------------- | - | - | - |

| [  ](https://arxiv.org/pdf/2403.20271) |   |   |   |
| -------------------------------------- | - | - | - |

| [  ](https://arxiv.org/pdf/2403.20271) | ArXiv 2024   | [Github](https://github.com/AFeng-x/Draw-and-Understand)        |                                                                                                                                                        |            |                                                                                       |   |
| -------------------------------------- | ------------ | --------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------- | ------------------------------------------------------------------------------------- | - |
| 2023-11-01                             | ChEF         | OpenGVLab                                                       | [CHEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models        ](https://arxiv.org/pdf/2311.02692) | ArXiv 2023 | [Github        ](https://github.com/OpenGVLab/LAMM)                                   |   |
| 2024-08-01                             | UniBench     | Facebook                                                        | [UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling        ](https://arxiv.org/pdf/2408.04810)                              | ArXiv 2024 | [Github        ](https://github.com/facebookresearch/unibench)                        |   |
| 2024-06-01                             | MME          | SJTU                                                            | [MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models        ](https://arxiv.org/pdf/2306.13394)                             | ArXiv 2024 | [Github        ](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) |   |
| 2023-08-01                             | MM-Vet       | Sun Yat-sen                                                     | [MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities        ](https://arxiv.org/pdf/2308.02490)                                     | ArXiv 2023 | [Github        ](https://github.com/yuweihao/MM-Vet)                                  |   |
| 2023-08-01                             | TouchStone   | OFA-Sys                                                         | [TouchStone: Evaluating Vision-Language Models by Language Models        ](https://arxiv.org/abs/2308.16890)                                           | ArXiv 2023 | [Github        ](https://github.com/OFA-Sys/TouchStone)                               |   |
| 2024-07-01                             | MMBench      | Open Compass                                                    | [MMBench: Is Your Multi-modal Model an All-around Player?        ](https://arxiv.org/pdf/2307.06281)                                                   | ArXiv 2024 | [Github        ](https://github.com/open-compass/VLMEvalKit)                          |   |
| 2024-04-01                             | OwlEval      | X-PLUG                                                          | [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality        ](https://arxiv.org/abs/2304.14178)                                | ArXiv 2024 | [Github        ](https://github.com/X-PLUG/mPLUG-Owl)                                 |   |
| 2023-07-01                             | Open-VQA     | Lynx                                                            | [What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?        ](https://arxiv.org/abs/2307.02469)                               | ArXiv 2023 | [Github        ](https://lynx-llm.github.io/)                                         |   |
| 2023-07-01                             | SEED-Bench   | AI Lab-CVC                                                      | [SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension        ](https://arxiv.org/abs/2307.16125)                                     | ArXiv 2023 | [Github        ](https://github.com/AILab-CVC/SEED-Bench)                             |   |
| 2023-11-01                             | SEED-Bench-2 | AI Lab-CVC                                                      | [SEED-Bench-2: Benchmarking Multimodal Large Language Models        ](https://arxiv.org/abs/2311.17092)                                                | ArXiv 2023 | [Github        ](https://github.com/AILab-CVC/SEED-Bench)                             |   |
| 2023-04-01                             | LLaVA-Bench  | LLAVA                                                           | [Visual Instruction Tuning        ](https://arxiv.org/abs/2304.08485)                                                                                  | ArXiv 2023 | [Github        ](https://llava-vl.github.io/)                                         |   |
| 2023-06-01                             | LAMM         | OpenGVLab                                                       | [LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark        ](https://arxiv.org/abs/2306.06687)                   | ArXiv 2023 | [Github        ](https://github.com/OpenGVLab/LAMM)                                   |   |

## Fine-grained Perception

### Visual Grounding and Object Detection

| Date       | Keywords           | Institute (first) | Paper                                                                                                                                           | Publication | Code                                                                 | Project |
| ---------- | ------------------ | ----------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | -------------------------------------------------------------------- | ------- |
| 2023-05-01 | CODE               | NUS               | [Contextual Object Detection with Multimodal Large Language Models        ](https://arxiv.org/abs/2305.18279)                                   | ArXiv 2023  | [Github        ](https://github.com/yuhangzang/ContextDET)           |         |
| 2016-05-01 | Flickr30k Entities | Brown             | [Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models        ](https://arxiv.org/abs/1505.04870) | ArXiv 2016  | [Github        ](https://github.com/BryanPlummer/flickr30k_entities) |         |
| 2016-11-01 | Visual7W           | Stanford          | [Visual7W: Grounded Question Answering in Images        ](https://arxiv.org/abs/1511.03416)                                                     | CVPR 2016   | [Github        ](https://github.com/yukezhu/visual7w-toolkit)        |         |
| 2023-12-01 | V\*Bench           | Tencent           | [V\*: Guided Visual Search as a Core Mechanism in Multimodal LLMs        ](https://arxiv.org/abs/2312.14135)                                    | ArXiv 2023  | [Github        ](https://github.com/penghao-wu/vstar)                |         |
| 2023-12-01 | Grounding-Bench    | UX-Decoder        | [LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models        ](https://arxiv.org/abs/2312.02949)                                  | ArXiv 2023  | [Github        ](https://github.com/UX-Decoder/LLaVA-Grounding)      |         |

### Fine-grained Identification and Recognition

| Date       | Keywords       | Institute (first) | Paper                                                                                                                                                                     | Publication | Code                                                      | Project |
| ---------- | -------------- | ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | --------------------------------------------------------- | ------- |
| 2023-05-01 | GVT-Bench      | Tencent           | [What Makes for Good Visual Tokenizers for Large Language Models?        ](https://arxiv.org/abs/2305.12223)                                                              | ArXiv 2023  | [Github        ](https://github.com/TencentARC/GVT)       |         |
| 2023-12-01 | V\*Bench       | Tencent           | [V\*: Guided Visual Search as a Core Mechanism in Multimodal LLMs        ](https://arxiv.org/abs/2312.14135)                                                              | ArXiv 2023  | [Github        ](https://github.com/penghao-wu/vstar)     |         |
| 2024-01-01 | MMVP           | SJTU              | [Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs        ](https://arxiv.org/abs/2401.06209)                                                          | ArXiv 2024  | [Github        ](https://tsb0601.github.io/mmvp_blog/)    |         |
| 2024-06-01 | CV-Bench       | OpenCompass       | [Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs        ](https://arxiv.org/abs/2406.16860)                                                       | ArXiv 2024  | [Github        ](https://cambrian-mllm.github.io/)        |         |
| 2024-03-01 | P2GB           | SJTU              | [Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models        ](https://arxiv.org/abs/2403.19322)                                                      | ArXiv 2024  |                                                           |         |
| 2024-03-01 | Visual CoT     | DeepCS            | [Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning        ](https://arxiv.org/abs/2403.16999)   | ArXiv 2024  | [Github        ](https://github.com/deepcs233/Visual-CoT) |         |
| 2023-11-01 | MagnifierBench | NUS               | [OtterHD: A High-Resolution Multi-modality Model      ](https://arxiv.org/abs/2311.04219)                                                                                 | ArXiv 2023  | [Github        ](https://github.com/Luodian/Otter)        |         |
| 2024-08-01 | HR-Bench       | Tencent           | [Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models      ](https://arxiv.org/abs/2408.15556) | ArXiv 2024  | [Github      ](https://github.com/DreamMr/HR-Bench)       |         |
| 2024-08-01 | SPARK          | KAIST             | [SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models      ](https://arxiv.org/abs/2408.12114)                            | ArXiv 2024  | [Github      ](https://github.com/top-yun/SPARK)          |         |

## Nuanced Vision-Language Alignment

|  Date       | Keywords                  | Institute (first) | Paper                                                                                                                                           | Publication  | Code                                                                               | Project                                                   |
| :---------: | :-----------------------: | :---------------: | :--------------------------------------------------------------------------------------------------------------------------------------------: | :----------: | :--------------------------------------------------------------------------------: | :-------------------------------------------------------: |
| 2023        | Equivariant Similarity    | Tsinghua          | [Equivariant Similarity for Vision-Language Foundation Models](https://arxiv.org/abs/2303.14465)                                               | ICCV 2023    | [Code](https://github.com/Wangt-CN/EqBen)                                          |                                                           |
| 2024        | Spectrum Evaluation       | Peking University | [Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2402.11217)                  | arXiv 2024   | [Code](https://asclepius-med.github.io/)                                           |                                                           |
| 2023        | Bags-of-Words Analysis    | Stanford          | [When and why vision-language models behave like bags-of-words, and what to do about it?](https://arxiv.org/abs/2210.01936)                    | ICLR 2023    |                                                                                   |                                                           |
| 2023        | VL Evaluation Checklist   | NYU               | [VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations](https://arxiv.org/abs/2207.00221)         | arXiv 2023   | [Code](https://github.com/om-ai-lab/VL-CheckList)                                 |                                                           |
| 2022        | Compositionality Probing | Facebook          | [Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality](https://arxiv.org/abs/2204.03162)                       | CVPR 2022    | [Code](https://huggingface.co/datasets/facebook/winoground)                        |                                                           |
| 2023        | Bags-of-Words Analysis    | Stanford          | [When and why vision-language models behave like bags-of-words, and what to do about it?](https://arxiv.org/abs/2210.01936)                    | ICLR 2023    |                                                                                   |                                                           |

## Image Understanding

### Multi-Image Understanding

|  Date       | Keywords                  | Institute (first) | Paper                                                                                                                                           | Publication  | Code                                                                               | Project                                                   |
| :---------: | :-----------------------: | :---------------: | :--------------------------------------------------------------------------------------------------------------------------------------------: | :----------: | :--------------------------------------------------------------------------------: | :-------------------------------------------------------: |
| 2024        | Image Sequence Reasoning  | UMD               | [Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences](https://arxiv.org/abs/2401.10529)     | arXiv 2024   | [Code](https://github.com/umd-huang-lab/Mementos)                                |                                                           |
| 2024        | Long Context Reasoning    | Tsinghua          | [MileBench: Benchmarking MLLMs in Long Context](https://arxiv.org/abs/2404.18532)                                                              | arXiv 2024   | [Code](https://milebench.github.io/)                                             |                                                           |
| 2024        | Robust Multi-image        | Zhejiang Univ.    | [MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding](https://arxiv.org/abs/2406.09411)                                  | arXiv 2024   | [Code](https://muirbench.github.io/)                                             |                                                           |
| 2024        | Comparative Reasoning     | KAIST             | [CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs](https://arxiv.org/abs/2407.16837)                                           | arXiv 2024   | [Code](https://compbench.github.io/)                                             |                                                           |
| 2024        | Multi-image Understanding | PKU               | [MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models](https://arxiv.org/abs/2408.02718)                     | arXiv 2024   | [Code](https://mmiu-bench.github.io/)                                            |                                                           |

### Implication Understanding

|  Date       | Keywords                  | Institute (first) | Paper                                                                                                                                           | Publication  | Code                                                                               | Project                                                   |
| :---------: | :-----------------------: | :---------------: | :--------------------------------------------------------------------------------------------------------------------------------------------: | :----------: | :--------------------------------------------------------------------------------: | :-------------------------------------------------------: |
| 2024        | Implication Benchmark     | NTU               | [II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2406.05862)                | arXiv 2024   | [Code](https://huggingface.co/datasets/m-a-p/II-Bench)                            |                                                           |
| 2024        | Implicit Attribute Value  | MIT               | [ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction](https://arxiv.org/abs/2404.15592)  | ACL 2024     | [Code](https://github.com/HenryPengZou/ImplicitAVE)                              |                                                           |
| 2024        | Affective Behavior        | Berkeley          | [FABA-Bench: Facial Affective Behavior Analysis with Instruction Tuning](https://arxiv.org/abs/2404.05052)                                     | arXiv 2024   | [Code](https://johnx69.github.io/FABA/)                                          |                                                           |

### Image Quality and Aesthetics Perception

|  Date       | Keywords                  | Institute (first) | Paper                                                                                                                                           | Publication  | Code                                                                               | Project                                                   |
| :---------: | :-----------------------: | :---------------: | :--------------------------------------------------------------------------------------------------------------------------------------------: | :----------: | :--------------------------------------------------------------------------------: | :-------------------------------------------------------: |
| 2024        | Aesthetics Perception     | Shanghai JiaoTong | [AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception](https://arxiv.org/abs/2401.08276)          | arXiv 2024   | [Code](https://github.com/yipoh/AesBench)                                        |                                                           |
| 2024        | Unified Aesthetic Baseline| Tencent           | [UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark](https://arxiv.org/abs/2404.09619)                             | arXiv 2024   | [Code](https://github.com/KwaiVGI/Uniaa)                                         |                                                           |
| 2024        | Graphic Design Benchmark  | Tsinghua          | [DesignProbe: A Graphic Design Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2404.14801)                               | arXiv 2024   |                                                                                   |                                                           |
| 2024        | Low-level Vision          | CUHK              | [Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision](https://arxiv.org/abs/2309.14181)                             | arXiv 2024   | [Code](https://q-future.github.io/Q-Bench/)                                     |                                                           |
| 2024        | Low-level Vision (Extended)| CUHK             | [Q-Bench+: A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs](https://arxiv.org/abs/2402.07116)   | TPAMI        | [Code](https://github.com/Q-Future/Q-Bench)                                     |                                                           |


## Cognition & Reasoning

### General Reasoning

| Date |                      Keywords                       | Institute (first) | Paper                                                                                                                                                           |   Publication   | Code                                                                    | Project |
| :--: | :-----------------------------------------------: | :--------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------: | :-----------------------------------------------------------------------: | :-----: |
| 2024 | Cognition & Reasoning / General Reasoning         | Unknown          | [MMRel: A Relation Understanding Dataset and Benchmark in the MLLM Era](https://arxiv.org/abs/2406.09121)                                                      | arXiv 2024      | [Github](https://github.com/niejiahao1998/MMRel)                         |         |
| 2023 | Cognition & Reasoning / General Reasoning         | Unknown          | [What's "up" with vision-language models? Investigating their struggle with spatial reasoning](https://arxiv.org/abs/2310.19785)                               | EMNLP 2023      | [Github](https://github.com/amitakamath/whatsup_vlms)                    |         |
| 2024 | Cognition & Reasoning / General Reasoning         | Unknown          | [GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs](https://arxiv.org/abs/2406.13246)                                       | arXiv 2024      | [Github]()                                                               |         |
| 2024 | Cognition & Reasoning / General Reasoning         | Unknown          | [The All-Seeing Project V2: Towards General Relation Comprehension of the Open World](https://arxiv.org/abs/2402.19474)                                        | ECCV 2024       | [Github](https://github.com/OpenGVLab/all-seeing)                        |         |
| 2022 | Cognition & Reasoning / General Reasoning         | Unknown          | [Visual Spatial Reasoning](https://arxiv.org/abs/2205.00363)                                                                                                   | arXiv 2022      | [Github](https://github.com/cambridgeltl/visual-spatial-reasoning)       |         |
| 2024 | Cognition & Reasoning / General Reasoning         | Unknown          | [SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model](https://arxiv.org/abs/2406.01584)                                                           | arXiv 2024      | [Github](https://www.anjiecheng.me/SpatialRGPT)                           |         |
| 2024 | Cognition & Reasoning / General Reasoning         | Unknown          | [Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images](https://arxiv.org/abs/2408.08105) | arXiv 2024      | [Github](https://github.com/Zhiyuan-Li-John/MuCR)                         |         |

---

### Context-dependent Reasoning

| Date |                   Keywords                    | Institute (first) | Paper                                                                                                                                       | Publication  | Code                                                 | Project |
| :--: | :------------------------------------------: | :--------------: | :------------------------------------------------------------------------------------------------------------------------------------------ | :----------: | :---------------------------------------------------: | :-----: |
| 2024 | Cognition & Reasoning / Context-dependent    | Unknown          | [CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models](https://arxiv.org/abs/2402.13607)         | arXiv 2024   | [Github](https://thunlp-mt.github.io/CODIS/)          |         |
| 2024 | Cognition & Reasoning / Context-dependent    | Unknown          | [Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models](https://arxiv.org/abs/2404.12966) | arXiv 2024   | [Github]()                                           |         |
| 2023 | Cognition & Reasoning / Context-dependent    | Unknown          | [VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning](https://arxiv.org/abs/2403.13164)                   | arXiv 2023   | [Github](https://github.com/ys-zong/VL-ICL)           |         |

---

### CoT Reasoning

| Date |               Keywords               | Institute (first) | Paper                                                                                                                                             | Publication | Code                                                       | Project |
| :--: | :---------------------------------: | :--------------: | :------------------------------------------------------------------------------------------------------------------------------------------------ | :---------: | :---------------------------------------------------------: | :-----: |
| 2022 | Cognition & Reasoning / CoT         | Unknown          | [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://arxiv.org/abs/2209.09513)                      | NIPS 2022   | [Github](https://scienceqa.github.io/)                     |         |
| 2024 | Cognition & Reasoning / CoT         | Unknown          | [Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning](https://arxiv.org/abs/2403.16999) | arXiv 2024  | [Github](https://github.com/deepcs233/Visual-CoT)          |         |
| 2024 | Cognition & Reasoning / CoT         | Unknown          | [M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought](https://arxiv.org/abs/2405.16473)                             | ACL 2024    | [Github](https://github.com/LightChen233/M3CoT)            |         |

---

### Vision-Indispensable Capabilities

| Date |                   Keywords                   | Institute (first) | Paper                                                                                                                                             | Publication | Code                                                      | Project |
| :--: | :-----------------------------------------: | :--------------: | :------------------------------------------------------------------------------------------------------------------------------------------------ | :---------: | :--------------------------------------------------------: | :-----: |
| 2016 | Cognition & Reasoning / Vision-Indispensable | Unknown          | [CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning](https://arxiv.org/abs/1612.06890)                        | arXiv 2016  | [Github](https://github.com/elinorp-d/clevr)               |         |
| 2024 | Cognition & Reasoning / Vision-Indispensable | Unknown          | [Visually Dehallucinative Instruction Generation: Know What You Don't Know](https://arxiv.org/abs/2402.09717)                                    | arXiv 2024  | [Github](https://github.com/ncsoft/idk)                    |         |
| 2019 | Cognition & Reasoning / Vision-Indispensable | Unknown          | [GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](https://arxiv.org/abs/1902.09506)                       | CVPR 2019   | [Github]()                                                |         |
| 2024 | Cognition & Reasoning / Vision-Indispensable | Unknown          | [Are We on the Right Way for Evaluating Large Vision-Language Models?](https://arxiv.org/abs/2403.20330)                                         | arXiv 2024  | [Github](https://mmstar-benchmark.github.io/)              | [Project](https://mmstar-benchmark.github.io/) |

---

## Knowledge-based Reasoning

### Knowledge-based Visual Question Answering

| Date |                 Keywords                  | Institute (first) | Paper                                                                                                                                  | Publication | Code                                                       | Project |
| :--: | :--------------------------------------: | :--------------: | :------------------------------------------------------------------------------------------------------------------------------------- | :---------: | :---------------------------------------------------------: | :-----: |
| 2015 | Knowledge-based VQA                      | Unknown          | [Explicit Knowledge-based Reasoning for Visual Question Answering](https://arxiv.org/abs/1511.02570)                                   | arXiv 2015  |                                                             |         |
| 2016 | Knowledge-based VQA                      | Unknown          | [FVQA: Fact-based Visual Question Answering](https://arxiv.org/abs/1606.05433)                                                         | arXiv 2016  | [Github](https://github.com/wangpengnorman/FVQA)           |         |
| 2019 | Knowledge-based VQA                      | Unknown          | [OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge](https://arxiv.org/abs/1906.00067)                         | CVPR 2019   | [Github](https://okvqa.allenai.org/)                       |         |
| 2022 | Knowledge-based VQA                      | Unknown          | [A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge](https://arxiv.org/abs/2206.01718)                           | arXiv 2022  | [Github](https://github.com/allenai/aokvqa)                |         |
| 2024 | Knowledge-based VQA                      | Unknown          | [SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge](https://arxiv.org/abs/2405.09713)                  | CVPR 2024   | [Github](https://bobbywu.com/SOKBench/)                    |         |

---

### Knowledge Editing

| Date |         Keywords         | Institute (first) | Paper                                                                                                                                         | Publication   | Code                                          | Project |
| :--: | :----------------------: | :--------------: | :-------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: | :--------------------------------------------: | :-----: |
| 2023 | Knowledge Editing        | Unknown          | [Can We Edit Multimodal Large Language Models?](https://arxiv.org/abs/2310.08475)                                                             | EMNLP 2023    | [Github](https://github.com/zjunlp/EasyEdit)   |         |
| 2024 | Knowledge Editing        | Unknown          | [MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing](https://arxiv.org/abs/2402.14835)                                 | arXiv 2024    |                                                |         |
| 2024 | Knowledge Editing        | Unknown          | [VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark](https://arxiv.org/abs/2403.07350)                                          | arXiv 2024    | [Github](https://github.com/VLKEB/VLKEB)       |         |
| 2024 | Knowledge Editing        | Unknown          | [MC-MKE: A Fine-Grained Multimodal Knowledge Editing Benchmark Emphasizing Modality Consistency](https://arxiv.org/abs/2406.13219)            | arXiv 2024    |                                                |         |



## Intelligence & Cognition

|    Date    |             Keywords              | Institute (first) | Paper                                                                                                                                                 |   Publication   | Code                                                                                               | Project                                                              |
|:----------:|:--------------------------------:|:-----------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------:|:----------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------:|
| 2019-06-00 | Visual Reasoning, AVR, Dataset   | N/A              | [RAVEN: A Dataset for Relational and Analogical Visual rEasoNing](https://arxiv.org/abs/1903.02741)                                                   | CVPR 2019       | -                                                                                                   | [Link](https://wellyzhang.github.io/project/raven.html)             |
| 2024-04-00 | Multidimensional Abstraction     | N/A              | [MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning](https://arxiv.org/abs/2404.13591)                         | arXiv 2024      | [Code](https://github.com/1171-jpg/MARVEL_AVR)                                                        | -                                                                    |
| 2024-06-00 | Multimodal, Visual Cognition     | N/A              | [VCog-Bench: What is the Visual Cognition Gap between Humans and Multimodal LLMs?](https://arxiv.org/abs/2406.10424)                                  | arXiv 2024      | [Code](https://github.com/IrohXu/VCog-Bench)                                                          | -                                                                    |
| 2024-06-00 | Multilingual, Multimodal, Cognition | N/A           | [M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark](https://arxiv.org/abs/2406.05343)                    | arXiv 2024      | -                                                                                                   | -                                                                    |

---

## Mathematical Question Answering

|    Date    |              Keywords              | Institute (first) | Paper                                                                                                                                                                                  |  Publication  | Code                                                                                               | Project                                                      |
|:----------:|:---------------------------------:|:-----------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------:|:----------------------------------------------------------------------------------------------------:|:------------------------------------------------------------:|
| 2024-01-00 | Visual Math Reasoning             | N/A              | [MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts](https://arxiv.org/abs/2310.02255)                                                                | ICLR 2024     | [Code](https://mathvista.github.io/)                                                                 | -                                                            |
| 2024-01-00 | Visual Math, Multi-modal          | N/A              | [MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?](https://arxiv.org/abs/2403.14624)                                                                | ECCV 2024     | [Code](https://mathverse-cuhk.github.io/)                                                            | -                                                            |
| 2024-01-00 | NP-hard, Dynamic Reasoning        | N/A              | [NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models](https://arxiv.org/abs/2403.01777)                                                                     | arXiv 2024    | [Code](https://github.com/lizhouf/NPHardEval4V)                                                      | -                                                            |
| 2024-01-00 | Multimodal Math Reasoning         | N/A              | [Math-Vision: Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset](https://arxiv.org/abs/2402.14804)                                                                   | arXiv 2024    | [Code](https://mathvision-cuhk.github.io/)                                                           | -                                                            |
| 2024-01-00 | Mathematical Reasoning, Checklist | N/A              | [MATHCHECK-GEO: Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist](https://arxiv.org/abs/2407.08733)                                          | arXiv 2024    | [Code](https://mathcheck.github.io/)                                                                 | -                                                            |
| 2021-06-00 | Geometry Problem Solving          | N/A              | [Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning](https://arxiv.org/abs/2105.04165)                                                       | ACL 2021      | [Code](https://lupantech.github.io/inter-gps/)                                                       | -                                                            |

---

## Multidisciplinary Question Answering

|    Date    |              Keywords             | Institute (first) | Paper                                                                                                                                                                                 |   Publication   | Code                                                                               | Project                                                     |
|:----------:|:--------------------------------:|:-----------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------:|:------------------------------------------------------------------------------------:|:-----------------------------------------------------------:|
| 2023-12-00 | Multi-lingual, Multi-modal       | N/A              | [M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models](https://arxiv.org/abs/2306.05179)                                                      | NeurIPS 2023    | [Code](https://github.com/DAMO-NLP-SG/M3Exam)                                        | -                                                           |
| 2024-01-00 | Chinese, Multi-discipline        | N/A              | [CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2401.11944)                                                                       | arXiv 2024      | [Code](https://cmmmu-benchmark.github.io/)                                            | -                                                           |
| 2022-12-00 | Science, Multi-modal Reasoning   | N/A              | [ScienceQA: Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://arxiv.org/abs/2209.09513)                                               | NeurIPS 2022    | [Code](https://scienceqa.github.io/)                                                  | -                                                           |
| 2024-06-00 | Multi-discipline, Multi-modal    | N/A              | [MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI](https://arxiv.org/abs/2311.16502)                                                  | CVPR 2024       | [Code](https://mmmu-benchmark.github.io/)                                             | -                                                           |
| 2024-01-00 | Chinese, Multi-modal QA          | N/A              | [CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning](https://arxiv.org/abs/2401.14011)                                                          | arXiv 2024      | [Code](https://github.com/FlagOpen/CMMU)                                               | -                                                           |
| 2024-01-00 | Scientific, College Entrance QA  | N/A              | [SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark](https://arxiv.org/abs/2402.05138)                                                                | arXiv 2024      | [Code](https://scemqa.github.io/)                                                    | -                                                           |
| 2024-01-00 | Text-Image, Multi-modal QA       | N/A              | [MULTI: Multimodal Understanding Leaderboard with Text and Images](https://arxiv.org/abs/2402.03173)                                                                                  | arXiv 2024      | [Code](https://opendfm.github.io/MULTI-Benchmark/)                                    | -                                                           |



### Specific Domains
#### Text-rich VQA
**Text-oriented Question Answering**
1. <mark>OCRBench</mark> **"On the Hidden Mystery of OCR in Large Multimodal Models"**. *LiuY, LiZ, HuangM, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2305.07895)] [[Github](https://github.com/Yuliang-Liu/MultimodalOCR)].
2. <mark>P2GB</mark> **"Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models"**. *ChenJ, LiuY, LiD, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2403.19322)] [[Github]()].
3. <mark>TextVQA</mark> **"Towards VQA Models That Can Read"**. *SinghA, NatarajanV, ShahM, et al.*. CVPR 2019. [[Paper](https://arxiv.org/abs/1904.08920)] [[Github](https://textvqa.org/)].
4. <mark>TextCaps</mark> **"TextCaps: a Dataset for Image Captioning with Reading Comprehension"**. *SidorovO, HuR, RohrbachM, et al.*. ECCV 2020. [[Paper](https://arxiv.org/abs/2003.12462)] [[Github](https://textvqa.org/textcaps/)].
5. <mark>SEED-Bench-2-Plus</mark> **"SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension"**. *Bohao Li, Yuying Ge, Yi Chen, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2404.16790)] [[Github](https://github.com/AILab-CVC/SEED-Bench)].
   
**Document-oriented Question Answering**
1. <mark>SPDocVQA</mark> **"Document Visual Question Answering Challenge 2020"**. *Minesh Mathew, Ruben Tito, Dimosthenis Karatzas, et al.*. DAS 2020. [[Paper](https://arxiv.org/abs/2008.08899)] [[Github]()].
2. <mark>MPDocVQA</mark> **"Hierarchical multimodal transformers for Multi-Page DocVQA"**. *Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny*. arXiv 2022. [[Paper](https://arxiv.org/abs/2212.05935)] [[Github](https://github.com/rubenpt91/MP-DocVQA-Framework)].
3. <mark>InfographicVQA</mark> **"Minesh Mathew and Viraj Bagal and Rubèn Pérez Tito and Dimosthenis Karatzas and Ernest Valveny and C. V Jawahar"**. *Minesh Mathew, Viraj Bagal, Rubèn Pérez Tito, et al.*. arXiv 2021. [[Paper](https://arxiv.org/abs/2104.12756)] [[Github](https://www.docvqa.org/)].
4. <mark>DUDE</mark> **"Document Understanding Dataset and Evaluation (DUDE)"**. *Jordy Van Landeghem, Rubén Tito, Łukasz Borchmann, et al.*. ICCV 2023. [[Paper](https://arxiv.org/abs/2305.08455)] [[Github](huggingface.co/datasets/jordyvl/DUDE_loader)].
5. <mark>MM-NIAH</mark> **"Needle In A Multimodal Haystack"**. *Weiyun Wang, Shuibo Zhang, Yiming Ren, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2406.07230)] [[Github](https://github.com/OpenGVLab/MM-NIAH)].

**Chart-oriented Question Answering**
1. <mark>ChartQA</mark> **"ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"**. *Ahmed Masry, Do Xuan Long, Jia Qing Tan, et al.*. ACL 2022. [[Paper](https://arxiv.org/abs/2203.10244)] [[Github]()].
2. <mark>ChartX</mark> **"ChartX and ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning"**. *Renqiu Xia, Bo Zhang, Hancheng Ye, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2402.12185)] [[Github](https://github.com/UniModal4Reasoning/ChartVLM)].
3. <mark>ChartBench</mark> **"ChartBench: A Benchmark for Complex Visual Reasoning in Charts"**. *Zhengzhuo Xu, Sinan Du, Yiyan Qi, et al.*. arXiv 2023. [[Paper](https://arxiv.org/abs/2312.15915)] [[Github](https://chartbench.github.io/)].
4. <mark>SciGraphQA</mark> **"SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs"**. *Shengzhi Li, Nima Tajbakhsh*. arXiv 2023. [[Paper](https://arxiv.org/abs/2308.03349)] [[Github](https://github.com/findalexli/SciGraphQA)].
5. <mark>MMC-Benchmark</mark> **"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning"**. *Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, et al.*. NAACL 2024. [[Paper](https://arxiv.org/abs/2311.10774)] [[Github](https://github.com/FuxiaoLiu/MMC)].
6. <mark>CharXiv</mark> **"CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs"**. *Zirui Wang, Mengzhou Xia, Luxi He, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2406.18521)] [[Github](https://charxiv.github.io/)].
7. <mark>CHOPINLLM</mark> **"On Pre-training of Multimodal Language Models Customized for Chart Understanding"**. *Wan-Cyuan Fan, Yen-Chun Chen, Mengchen Liu, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2407.14506)] [[Github]()].
8. <mark>SciFIBench</mark> **"SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation"**. *Jonathan Roberts, Kai Han, Neil Houlsby, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2405.08807)] [[Github](https://github.com/jonathan-roberts1/SciFIBench)].

**Html-oriented Question Answering**
1. <mark>Web2Code</mark> **"Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs"**. *Sukmin Yun, Haokun Lin, Rusiru Thushara, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2406.20098)] [[Github](https://mbzuai-llm.github.io/webpage2code/)].
2. <mark>VisualWebBench</mark> **"VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?"**. *Junpeng Liu, Yifan Song, Bill Yuchen Lin, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2404.05955)] [[Github](https://github.com/VisualWebBench/VisualWebBench)].
3. <mark>Plot2Code</mark> **"Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots"**. *Chengyue Wu, Yixiao Ge, Qiushan Guo, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2405.07990)] [[Github](https://huggingface.co/datasets/TencentARC/Plot2Code)].

#### Decision-making Agents
**Embodied Decision-making**
1. <mark>VisualAgentBench</mark> **"VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents"**. *Xiao Liu, Tianjie Zhang, Yu Gu, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2408.06327)] [[Github](https://github.com/THUDM/VisualAgentBench)].
2. <mark>EgoPlan-Bench</mark> **"EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning"**. *Yi Chen, Yuying Ge, Yixiao Ge, et al.*. arXiv 2023. [[Paper](https://arxiv.org/abs/2312.06722)] [[Github](https://github.com/ChenYi99/EgoPlan)].
3. <mark>PCA-EVAL</mark> **"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"**. *Liang Chen, Yichi Zhang, Shuhuai Ren, et al.*. arXiv 2023. [[Paper](https://arxiv.org/abs/2310.02071)] [[Github](https://github.com/pkunlp-icler/PCA-EVAL/)].
4. <mark>OpenEQA</mark> **"OpenEQA: Embodied Question Answering in the Era of Foundation Models"**. *Majumdar, Arjun and Ajay, Anurag and Zhang, et al.*. CVPR 2024. [[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf)] [[Github](https://open-eqa.github.io/)].
5. <mark>OSWorld</mark> **"OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments"**. *Tianbao Xie, Danyang Zhang, Jixuan Chenet al.*. NeurIPS 2024. [[Paper](https://arxiv.org/abs/2404.07972)] [[Github](https://os-world.github.io/)].
   
**Mobile Agency**
1. <mark>Mobile-Eval</mark> **"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception"**. *Junyang Wang, Haiyang Xu, Jiabo Ye, et al.*. ICLR 2024. [[Paper](https://arxiv.org/abs/2401.16158)] [[Github](https://github.com/X-PLUG/MobileAgent)].
2. <mark>Fereet-UI</mark> **"Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs"**. *You K, Zhang H, Schoop E, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2404.05719)] [[Github](https://github.com/apple/ml-ferret)].
3. <mark>CRAB</mark> **"CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents"**. *Tianqi Xu, Linyao Chen, Dai-Jie Wu, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2407.01511)] [[Github](https://github.com/camel-ai/crab)].
   
#### Diverse Cultures&Languages
1. <mark>CMMU</mark> **"CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning"**. *Zheqi He, Xinya Wu, Pengfei Zhou, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2401.14011)] [[Github](https://github.com/flageval-baai/CMMU)].
2. <mark>Henna</mark> **"Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks"**. *Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2403.01031)] [[Github](https://github.com/UBC-NLP/peacock)].
3. <mark>LaVy-Bench</mark> **"LaVy: Vietnamese Multimodal Large Language Model"**. *Chi Tran, Huong Le Thanh*. arXiv 2024. [[Paper](https://arxiv.org/abs/2404.07922)] [[Github](https://github.com/baochi0212/LaVy)].
4. <mark>MTVQA</mark> **"MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering"**. *Jingqun Tang, Qi Liu, Yongjie Ye, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2405.11985)] [[Github](https://bytedance.github.io/MTVQA/)].
5. <mark>CVQA</mark> **"CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark"**. *David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2406.05967)] [[Github](https://cvqa-benchmark.org/)].
6. <mark>CMMMU</mark> **"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark"**. *Ge Zhang, Xinrun Du, Bei Chen, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2401.11944)] [[Github](https://github.com/CMMMU-Benchmark/CMMMU)].
7. <mark>MULTI</mark> **"MULTI: Multimodal Understanding Leaderboard with Text and Images"**. *Zichen Zhu, Yang Xu, Lu Chen, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2402.03173)] [[Github](https://opendfm.github.io/MULTI-Benchmark/)].

#### Other Applications
**Geography and Remote Sensing**
1. <mark>LHRS-Bench</mark> **"LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model"**. *Dilxat Muhtar, Zhenshi Li, Feng Gu, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2402.02544)] [[Github](https://github.com/NJU-LHRS/LHRS-Bot)].
2. <mark>ChartingNewTerritories</mark> **"Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs"**. *Jonathan Roberts, Timo Lüddecke, Rehan Sheikh, et al.*. arXiv 2023. [[Paper](https://arxiv.org/abs/2311.14656)] [[Github](https://github.com/jonathan-roberts1/charting-new-territories)].
   
**Medicine**
1. <mark>GMAI-MMBench</mark> **"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI"**. *Pengcheng Chen, Jin Ye, Guoan Wang, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2408.03361)] [[Github](https://github.com/uni-medical/GMAI-MMBench)].
2. <mark>M3D</mark> **"M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts"**. *Mingsheng Li, Xin Chen, Chi Zhang, et al.*. arXiv 2023. [[Paper](https://arxiv.org/abs/2312.10763)] [[Github](https://github.com/OpenM3D/M3DBench)].
3. <mark>Asclepius</mark> **"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models"**. *Wenxuan Wang, Yihang Su, Jingyuan Huan, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2402.11217)] [[Github](https://asclepius-med.github.io/)].
4. <mark>MultiMed</mark> **"MultiMed: Massively Multimodal and Multitask Medical Understanding"**. *Shentong Mo, Paul Pu Liang*. arXiv 2024. [[Paper](https://arxiv.org/abs/2408.12682)] [[Github]()].
   
**Society**
1. <mark>VizWiz</mark> **"VizWiz Grand Challenge: Answering Visual Questions from Blind People"**. *Danna Gurari, Qing Li, Abigale J. Stangl, et al.*. arXiv 2018. [[Paper](https://arxiv.org/abs/1802.08218)] [[Github](https://github.com/DenisDsh/VizWiz-VQA-PyTorch)].
2. <mark>MM-Soc</mark> **"MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms"**. *Yiqiao Jin, Minje Choi, Gaurav Verma, et al.*. ACL 2024. [[Paper](https://arxiv.org/abs/2402.14154)] [[Github](https://github.com/claws-lab/MMSoc)].
3. <mark>TransportationGames</mark> **"TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models"**. *Xue Zhang, Xiangyu Shi, Xinyue Lou, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2401.04471)] [[Github](https://github.com/sxysxy/Transportation.Games)].
   
**Industry**
1. <mark>MMRo</mark> **"MMRo: Are Multimodal LLMs Eligible as the Brain for In-Home Robotics?"**. *Jinming Li, Yichen Zhu, Zhiyuan Xu, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2406.19693)] [[Github]()].
2. <mark>DesignQA</mark> **"DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation"**. *Anna C. Doris, Daniele Grandi, Ryan Tomich, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2404.07917)] [[Github](https://github.com/anniedoris/design_qa)].
   
**Autonomous Driving**
1. <mark>NuScenes-QA</mark> **"NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario"**. *Tianwen Qian, Jingjing Chen, Linhai Zhuo, et al.*. AAAI 2024. [[Paper](https://arxiv.org/abs/2305.14836)] [[Github](https://github.com/qiantianwen/NuScenes-QA)].
2. <mark>DriveLM-DATA</mark> **"DriveLM: Driving with Graph Visual Question Answering"**. *Chonghao Sima, Katrin Renz, Kashyap Chitta, et al.*. ECCV 2024. [[Paper](https://arxiv.org/abs/2312.14150)] [[Github](https://github.com/OpenDriveLab/DriveLM)].

### Key Capabilities
#### Conversation Abilities
**Long-context**
1. <mark>Mile-Bench</mark> **"MileBench: Benchmarking MLLMs in Long Context"**. *Song D, Chen S, Chen G H, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2404.18532)] [[Github](https://milebench.github.io/)].
2. <mark>MMNeedle</mark> **"Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models"**. *Wang H, Shi H, Tan S, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2406.11230)] [[Github](https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack)].
3. <mark>MLVU</mark> **"MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding"**. *Zhou J, Shu Y, Zhao B, et al.*. arXiv 2024. [[Paper](https://arxiv.org/abs/2406.04264)] [[Github](https://github.com/JUNJIE99/MLVU)].
   
**Instruction Following**
1. <mark>CoIN</mark> **"CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model"**. *Chen C, Zhu J, Luo X, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2403.08350)] [[Github](https://github.com/zackschen/CoIN)].
2. <mark>MIA-Bench</mark> **"MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs"**. *Qian Y, Ye H, Fauconnier J P, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2407.01509)] [[Github](https://github.com/apple/ml-mia-bench)].
3. <mark>DEMON</mark> **"Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"**. *Li J, Pan K, Ge Z, et al.*. ICLR 2023. [[Paper](https://arxiv.org/pdf/2308.04152)] [[Github](https://github.com/DCDmllm/Cheetah)].
4. <mark>VisIT-Bench</mark> **"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"**. *Bitton Y, Bansal H, Hessel J, et al.*. NeurIPS 2023. [[Paper](https://arxiv.org/pdf/2308.06595)] [[Github](https://visit-bench.github.io/)].

#### Hallucination
1. <mark>POPE</mark> **"Evaluating Object Hallucination in Large Vision-Language Models"**. *Li Y, Du Y, Zhou K, et al.*. EMNLP 2023. [[Paper](https://arxiv.org/pdf/2305.10355)] [[Github](https://github.com/RUCAIBox/POPE)].
2. <mark>GAVIE</mark> **"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"**. *Liu F, Lin K, Li L, et al.*. ICLR 2023. [[Paper](https://www.researchgate.net/profile/Fuxiao-Liu-2/publication/375596083_Mitigating_Hallucination_in_Large_Multi-Modal_Models_via_Robust_Instruction_Tuning/links/655188b0b1398a779d7f4a04/Mitigating-Hallucination-in-Large-Multi-Modal-Models-via-Robust-Instruction-Tuning.pdf)] [[Github](https://fuxiaoliu.github.io/LRV/)].
3. <mark>HaELM</mark> **"Evaluation and Analysis of Hallucination in Large Vision-Language Models"**. *Wang J, Zhou Y, Xu G, et al.*. arXiv 2023. [[Paper](https://arxiv.org/pdf/2308.15126)] [[Github](https://github.com/junyangwang0410/HaELM)].
4. <mark>M-HalDetect</mark> **"Detecting and Preventing Hallucinations in Large Vision Language Models"**. *Gunjal A, Yin J, Bas E.*. AAAI 2024. [[Paper](https://arxiv.org/pdf/2308.06394)] [[Github](https://github.com/hendryx-scale/mhal-detect)].
5. <mark>Bingo</mark> **"Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges"**. *Cui C, Zhou Y, Yang X, et al.*. arXiv 2023. [[Paper](https://arxiv.org/pdf/2311.03287)] [[Github](https://github.com/gzcch/Bingo)].
6. <mark>HallusionBench</mark> **"HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models"**. *Guan T, Liu F, Wu X, et al.*. CVPR 2024. [[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Guan_HallusionBench_An_Advanced_Diagnostic_Suite_for_Entangled_Language_Hallucination_and_CVPR_2024_paper.pdf)] [[Github](https://github.com/tianyi-lab/HallusionBench)].
7. <mark>VHTest</mark> **"Visual Hallucinations of Multi-modal Large Language Models"**. *Huang W, Liu H, Guo M, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2402.14683)] [[Github](https://github.com/wenhuang2000/VHTest)].
8. <mark>CorrelationQA</mark> **"The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs"**. *Han T, Lian Q, Pan R, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2402.03757)] [[Github](https://github.com/MasaiahHan/CorrelationQA)].
9. <mark>CHAIR</mark> **"Object Hallucination in Image Captioning"**. *Rohrbach A, Hendricks L A, Burns K, et al.*. EMNLP 2018. [[Paper](https://arxiv.org/pdf/1809.02156)] [[Github](https://github.com/LisaAnne/Hallucination)].
10. <mark>MHaluBench</mark> **"Unified Hallucination Detection for Multimodal Large Language Models"**. *Chen X, Wang C, Xue Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2402.03190)] [[Github](https://github.com/zjunlp/EasyDetect)].
11. <mark>VideoHallucer</mark> **"VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models"**. *Wang Y, Wang Y, Zhao D, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2406.16338)] [[Github](https://videohallucer.github.io/)].
12. <mark>MMHAL-BENCH</mark> **"Aligning Large Multimodal Models with Factually Augmented RLHF"**. *Sun Z, Shen S, Cao S, et al.*. arXiv 2023. [[Paper](https://arxiv.org/pdf/2309.14525)] [[Github](https://llava-rlhf.github.io/)].
13. <mark>AMBER</mark> **"AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation"**. *Wang J, Wang Y, Xu G, et al.*. arXiv 2023. [[Paper](https://arxiv.org/pdf/2311.07397)] [[Github](https://github.com/junyangwang0410/AMBER)].
14. <mark>MMECeption</mark> **"GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data"**. *Cao L, Buchner V, Senane Z, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2402.14973)] [[Github](https://github.com/llcresearch/GenCeption)].


   
#### Trustworthiness
**Robustness**
1. <mark>MAD-Bench</mark> **"How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts"**. *Qian Y, Zhang H, Yang Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2402.13220)] [[Github]()].
2. <mark>MMR</mark> **"Seeing Clearly, Answering Incorrectly: A Multimodal Robustness Benchmark for Evaluating MLLMs on Leading Questions"**. *Liu Y, Liang Z, Wang Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2406.10638)] [[Github](https://github.com/BAAI-DCAI/Multimodal-Robustness-Benchmark)].
3. <mark>MM-SpuBench</mark> **"MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs"**. *Ye W, Zheng G, Ma Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2406.17126)] [[Github](https://huggingface.co/datasets/mmbench/MM-SpuBench)].
4. <mark>MM-SAP</mark> **"MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception"**. *Wang Y, Liao Y, Liu H, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2401.07529)] [[Github](https://github.com/YHWmz/MM-SAP)].
5. <mark>BenchLMM</mark> **"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"**. *Cai R, Song Z, Guan D, et al.*. arXiv 2023. [[Paper](https://arxiv.org/pdf/2312.02896)] [[Github](https://github.com/AIFEG/BenchLMM)].
6. <mark>VQAv2-IDK</mark> **"Visually Dehallucinative Instruction Generation: Know What You Don’t Know"**. *Cha S, Lee J, Lee Y, et al.*. ICASSP 2024. [[Paper](https://arxiv.org/pdf/2402.09717)] [[Github](https://github.com/ncsoft/idk)].
7. 
**Safety**
1. <mark>MMUBench</mark> **"Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models"**. *Li J, Wei Q, Zhang C, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2405.12523)] [[Github]()].
2. <mark>JailBreakV-28K</mark> **"JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks"**. *Luo W, Ma S, Liu X, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2404.03027)] [[Github](https://eddyluo1232.github.io/JailBreakV28K/)].
3. <mark>MultiTrust</mark> **"Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study"**. *Zhang Y, Huang Y, Sun Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2406.07057)] [[Github](https://multi-trust.github.io/)].
4. <mark>MM-SafetyBench</mark> **"MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"**. *Liu X, Zhu Y, Gu J, et al.*. ECCV 2024. [[Paper](https://arxiv.org/pdf/2311.17600)] [[Github](https://github.com/isXinLiu/MM-SafetyBench)].
5. <mark>SHIELD</mark> **"SHIELD: An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models"**. *Shi Y, Gao Y, Lai Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2402.04178)] [[Github](https://github.com/laiyingxin2/SHIELD)].
6. <mark>RTVLM</mark> **"Red teaming visual language models"**. *Li M, Li L, Yin Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2401.12915)] [[Github](https://huggingface.co/datasets/MMInstruction/RedTeamingVLM)].

### Other Modalities
#### Videos
**Temporal Perception**
1. <mark>MVBench</mark> **"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"**. *Li K, Wang Y, He Y, et al.*. CVPR 2024. [[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.pdf)] [[Github](https://github.com/OpenGVLab/Ask-Anything)].
2. <mark>TimeIT</mark> **"Timechat: A time-sensitive multimodal large language model for long video understanding"**. *Ren S, Yao L, Li S, et al.*. CVPR 2024. [[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_TimeChat_A_Time-sensitive_Multimodal_Large_Language_Model_for_Long_Video_CVPR_2024_paper.pdf)] [[Github](https://github.com/RenShuhuai-Andy/TimeChat)].
3. <mark>ViLMA</mark> **"ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models"**. *Kesen I, Pedrotti A, Dogan M, et al.*. ICLR 2024. [[Paper](https://arxiv.org/pdf/2311.07022)] [[Github](https://cyberiada.github.io/ViLMA/)].
4. <mark>VITATECS</mark> **"VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models"**. *Li S, Li L, Ren S, et al.*. arXiv 2023. [[Paper](https://arxiv.org/pdf/2311.17404)] [[Github](https://github.com/lscpku/VITATECS)].
5. <mark>TempCompass</mark> **"TempCompass: Do Video LLMs Really Understand Videos?"**. *Liu Y, Li S, Liu Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2403.00476)] [[Github](https://github.com/llyx97/TempCompass)].
6. <mark>OSCaR</mark> **"OSCaR: Object State Captioning and State Change Representation"**. *Nguyen N, Bi J, Vosoughi A, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2402.17128)] [[Github](https://github.com/nguyennm1024/OSCaR)].
7. <mark>ADLMCQ</mark> **"LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living"**. *Chakraborty R, Sinha A, Reilly D, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2406.09390)] [[Github](https://github.com/ADL-X/LLAVIDAL)].
8. <mark>Perception Test</mark> **"Perception Test: A Diagnostic Benchmark for Multimodal Video Models"**. *Patraucean V, Smaira L, Gupta A, et al.*. NeurIPS2024. [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/8540fba4abdc7f9f7a7b1cc6cd60e409-Paper-Datasets_and_Benchmarks.pdf)] [[Github](https://github.com/google-deepmind/perception_test)].

**Long Video Understanding**
1. <mark>MovieChat-1k</mark> **"Moviechat: From dense token to sparse memory for long video understanding"**. **. . [[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.pdf)] [[Github](https://github.com/rese1f/MovieChat)].
2. <mark>EgoSchema</mark> **"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding"**. **. . [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/90ce332aff156b910b002ce4e6880dec-Paper-Datasets_and_Benchmarks.pdf)] [[Github](https://egoschema.github.io/)].
3. <mark>Event-Bench</mark> **"Towards Event-oriented Long Video Understanding"**. **. . [[Paper](https://arxiv.org/pdf/2406.14129)] [[Github](https://github.com/RUCAIBox/Event-Bench)].
4. <mark>MLVU</mark> **"MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding"**. **. . [[Paper](https://arxiv.org/abs/2406.04264)] [[Github](https://github.com/JUNJIE99/MLVU)].

**Comprehensive Evaluation**
1. <mark>Video-Bench</mark> **"Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models"**. *Ning M, Zhu B, Xie Y, et al.*. arXiv 2023. [[Paper](https://arxiv.org/abs/2311.16103)] [[Github](https://github.com/PKU-YuanGroup/Video-Bench)].
2. <mark>MMBench-Video</mark> **"MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding"**. *Fang X, Mao K, Duan H, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2406.14515)] [[Github](https://github.com/open-compass/VLMEvalKit)].
3. <mark>Video-MME</mark> **"Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis"**. *Fu C, Dai Y, Luo Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2405.21075)] [[Github](https://video-mme.github.io/)].
4. <mark>AutoEval-Video</mark> **"AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering"**. *Chen X, Lin Y, Zhang Y, et al.*. arXiv 2023. [[Paper](https://arxiv.org/pdf/2311.14906)] [[Github](https://github.com/Xiuyuan-Chen/AutoEval-Video)].
5. <mark>MMWorld</mark> **"MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos"**. *He X, Feng W, Zheng K, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2406.08407)] [[Github](https://mmworld-bench.github.io/)].
6. <mark>WorldNet</mark> **"WorldGPT: Empowering LLM as Multimodal World Model"**. *Ge Z, Huang H, Zhou M, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2404.18202)] [[Github](https://github.com/DCDmllm/WorldGPT)].


#### Audio
1. <mark>Dynamic-SUPERB</mark> **"Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech"**. *Huang C, Lu K H, Wang S H, et al.*. ICASSP 2024. [[Paper](https://arxiv.org/pdf/2309.09510)] [[Github](https://github.com/dynamic-superb/dynamic-superb)].
2. <mark>MuChoMusic</mark> **"MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models"**. *Weck B, Manco I, Benetos E, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2408.01337)] [[Github](https://mulab-mir.github.io/muchomusic/)].
3. <mark>AIR-Bench</mark> **"AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension"**. *Yang Q, Xu J, Liu W, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2402.07729)] [[Github](https://github.com/OFA-Sys/AIR-Bench)].

#### 3D Points
1. <mark>ScanQA</mark> **"ScanQA: 3D Question Answering for Spatial Scene Understanding"**. *Azuma D, Miyanishi T, Kurita S, et al.*. CVPR 2022. [[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Azuma_ScanQA_3D_Question_Answering_for_Spatial_Scene_Understanding_CVPR_2022_paper.pdf)] [[Github](https://github.com/ATR-DBI/ScanQA)].
2. <mark>ScanReason</mark> **"ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities"**. *Zhu C, Wang T, Zhang W, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2407.01525)] [[Github](https://zcmax.github.io/projects/ScanReason/)].
3. <mark>LAMM</mark> **"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"**. *Yin Z, Wang J, Cao J, et al.*. NeurIPS 2024. [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/548a41b9cac6f50dccf7e63e9e1b1b9b-Paper-Datasets_and_Benchmarks.pdf)] [[Github](https://openlamm.github.io/)].
4. <mark>SpatialRGPT</mark> **"SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model"**. *Cheng A C, Yin H, Fu Y, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2406.01584)] [[Github](https://www.anjiecheng.me/SpatialRGPT)].
5. <mark>M3DBench</mark> **"M3DBench: Let’s Instruct Large Models with Multi-modal 3D Prompts"**. *Li M, Chen X, Zhang C, et al.*. arXiv 2023. [[Paper](https://arxiv.org/pdf/2312.10763)] [[Github](https://github.com/OpenM3D/M3DBench/)].
   
#### Omni-modal
1. <mark>MCUB</mark> **"Model Composition for Multimodal Large Language Models"**. *Chen C, Du Y, Fang Z, et al.*. arXiv 2024. [[Paper](https://arxiv.org/pdf/2402.12750)] [[Github](https://github.com/THUNLP-MT/ModelCompose)].
2. <mark>AVQA</mark> **"AVQA: A Dataset for Audio-Visual Question Answering on Videos"**. *Yang P, Wang X, Duan X, et al.*. MM 2022. [[paper](https://dl.acm.org/doi/pdf/10.1145/3503161.3548291)] [[Github](https://mn.cs.tsinghua.edu.cn/avqa/)].
3. <mark>MusicAVQA</mark> **"Learning to Answer Questions in Dynamic Audio-Visual Scenarios"**. *Li G, Wei Y, Tian Y, et al.*. CVPR 2022. [[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.pdf)] [[Github](https://gewu-lab.github.io/MUSIC-AVQA/)].
4. <mark>MMT-Bench</mark> **"MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI"**. *Ying K, Meng F, Wang J, et al.*. arXiv 2024. [[paper](https://arxiv.org/pdf/2404.16006)] [[Github](https://github.com/OpenGVLab/MMT-Bench)].
